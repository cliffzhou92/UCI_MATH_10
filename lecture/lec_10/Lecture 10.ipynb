{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 10 Introduction to Machine Learning and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example I: Single-variable (1D) Linear Regression\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $(x^{(i)}\\in\\mathbb{R},y^{(i)}\\in\\mathbb{R}), i= 1,2,..., N$, we want to find the linear function $$y\\approx f(x)=wx +b$$ that fits the relations between $x^{(i)}$ and $y^{(i)}$. So that given any new $x^{test}$ in the **test** dataset, we can make the prediction $$y^{pred} = w x^{test}+b$$\n",
    "\n",
    "### Training the model\n",
    "\n",
    "- With the training dataset, define the loss function $L(w,b)$ of parameter $w$ and $b$, which is also called **mean squared error** (MSE) $$L(w,b)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y^{(i)}\\big)^2=\\frac{1}{N}\\sum_{i=1}^N\\big((wx^{(i)}+b)-y^{(i)}\\big)^2,$$ where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x^{(i)}$, i.e. $\\hat{y}^{(i)} = wx^{(i)}+b$.\n",
    "\n",
    "\n",
    "- Then find the minimum of loss function -- note that this is the quadratic function of $w$ and $b$, and we can analytically solve $\\partial_{w}L = \\partial_{b}L =0$, and yields\n",
    "\n",
    "$$ w^* =\\frac{\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})(y^{(i)}-\\bar{y})}{\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})^{2}} = \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})(y^{(i)}-\\bar{y})}{\\frac{1}{N}\\sum_{i=1}^{N}(x^{(i)}-\\bar{x})^{2}} = \\frac{\\text{Cov}(X,Y)}{\\text{Var}(X)},$$\n",
    "\n",
    "$$ b^* = \\bar{y}  - w^*\\bar{x},$$\n",
    "\n",
    "where $\\bar{x}$ and $\\bar{y}$ are the mean of $x$ and of $y$, and $\\text{Cov}(X,Y)$ denotes the estimated covariance (or called sample covariance) between $X$ and $Y$ (a little difference with what you learned in statistics is that we have the normalization factor $1/N$ instead of $1/(N-1)$ here), and $\\text{Var}(Y)$ denotes the sample variance of $Y$ (the normalization factor is still $1/N$). This is just about convention -- in statistics, they pursue for unbiased estimator.\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class MyLinearRegression1D:\n",
    "    '''\n",
    "    The single-variable linear regression estimator -- writing in the style of sklearn package\n",
    "    '''\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        '''\n",
    "        Determine the optimal parameters w, b for the input data x and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array with shape (n_samples,) from training data\n",
    "           y : 1D numpy array with shape (n_samples,) from training data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : returns an instance of self, with new attributes slope w (float) and intercept b (float)\n",
    "         '''\n",
    "        \n",
    "        cov_mat = np.cov(x,y,bias=True) # covariance matrix, bias = True makes the factor is 1/N -- but it doesn't matter actually, since the factor will be cancelled\n",
    "        self.w = cov_mat[0,1] / cov_mat[0,0] # the (0,1) element is COV(X,Y) and (0,0) element is Var(X). (1,1) is Var(Y)\n",
    "        self.b =  np.mean(y)-self.w * np.mean(x)\n",
    "        \n",
    "    def predict(self,x):\n",
    "        '''\n",
    "        Predict the output values for the input value x, based on trained parameters\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array from training or test data \n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns 1D numpy array of same shape as input, the predicted y value of corresponding x\n",
    "        '''\n",
    "        \n",
    "        return self.w*x+self.b\n",
    "    \n",
    "    def score(self, x, y):\n",
    "        '''\n",
    "        Calculate the R-squared on the dataset with input x and y\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "           x : 1D numpy array with shape (n_samples,) from training or test data\n",
    "           y : 1D numpy array with shape (n_samples,) from training or test data\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        returns float, the R^2 value\n",
    "        '''\n",
    "        \n",
    "        y_hat = self.predict (x) # predicted y\n",
    "        mse = np.mean((y-y_hat)**2) # mean squared error\n",
    "        return 1- mse / np.var(y) # return R-squared\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>...</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>yr_built</th>\n",
       "      <th>yr_renovated</th>\n",
       "      <th>zipcode</th>\n",
       "      <th>lat</th>\n",
       "      <th>long</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>16792</th>\n",
       "      <td>6388930420</td>\n",
       "      <td>20140805T000000</td>\n",
       "      <td>582000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2380</td>\n",
       "      <td>19860</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2380</td>\n",
       "      <td>0</td>\n",
       "      <td>1995</td>\n",
       "      <td>0</td>\n",
       "      <td>98056</td>\n",
       "      <td>47.5255</td>\n",
       "      <td>-122.173</td>\n",
       "      <td>2450</td>\n",
       "      <td>10220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7648</th>\n",
       "      <td>7625703945</td>\n",
       "      <td>20140701T000000</td>\n",
       "      <td>345000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1080</td>\n",
       "      <td>7775</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>6</td>\n",
       "      <td>1080</td>\n",
       "      <td>0</td>\n",
       "      <td>1955</td>\n",
       "      <td>0</td>\n",
       "      <td>98136</td>\n",
       "      <td>47.5447</td>\n",
       "      <td>-122.394</td>\n",
       "      <td>1730</td>\n",
       "      <td>7350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9653</th>\n",
       "      <td>1402600110</td>\n",
       "      <td>20150226T000000</td>\n",
       "      <td>392000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2360</td>\n",
       "      <td>7733</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2360</td>\n",
       "      <td>0</td>\n",
       "      <td>1983</td>\n",
       "      <td>0</td>\n",
       "      <td>98058</td>\n",
       "      <td>47.4403</td>\n",
       "      <td>-122.137</td>\n",
       "      <td>2160</td>\n",
       "      <td>7733</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21563</th>\n",
       "      <td>9406530090</td>\n",
       "      <td>20141020T000000</td>\n",
       "      <td>337000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2470</td>\n",
       "      <td>5100</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>8</td>\n",
       "      <td>2470</td>\n",
       "      <td>0</td>\n",
       "      <td>2005</td>\n",
       "      <td>0</td>\n",
       "      <td>98038</td>\n",
       "      <td>47.3622</td>\n",
       "      <td>-122.041</td>\n",
       "      <td>2240</td>\n",
       "      <td>5123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19437</th>\n",
       "      <td>5379803386</td>\n",
       "      <td>20140801T000000</td>\n",
       "      <td>289950.0</td>\n",
       "      <td>4</td>\n",
       "      <td>1.75</td>\n",
       "      <td>1500</td>\n",
       "      <td>8400</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>7</td>\n",
       "      <td>1200</td>\n",
       "      <td>300</td>\n",
       "      <td>1956</td>\n",
       "      <td>0</td>\n",
       "      <td>98188</td>\n",
       "      <td>47.4531</td>\n",
       "      <td>-122.273</td>\n",
       "      <td>1780</td>\n",
       "      <td>9913</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               id             date     price  bedrooms  bathrooms  \\\n",
       "16792  6388930420  20140805T000000  582000.0         3       2.50   \n",
       "7648   7625703945  20140701T000000  345000.0         2       1.00   \n",
       "9653   1402600110  20150226T000000  392000.0         4       2.25   \n",
       "21563  9406530090  20141020T000000  337000.0         4       2.50   \n",
       "19437  5379803386  20140801T000000  289950.0         4       1.75   \n",
       "\n",
       "       sqft_living  sqft_lot  floors  waterfront  view  ...  grade  \\\n",
       "16792         2380     19860     2.0           0     0  ...      8   \n",
       "7648          1080      7775     1.0           0     0  ...      6   \n",
       "9653          2360      7733     2.0           0     0  ...      8   \n",
       "21563         2470      5100     2.0           0     0  ...      8   \n",
       "19437         1500      8400     1.0           0     0  ...      7   \n",
       "\n",
       "       sqft_above  sqft_basement  yr_built  yr_renovated  zipcode      lat  \\\n",
       "16792        2380              0      1995             0    98056  47.5255   \n",
       "7648         1080              0      1955             0    98136  47.5447   \n",
       "9653         2360              0      1983             0    98058  47.4403   \n",
       "21563        2470              0      2005             0    98038  47.3622   \n",
       "19437        1200            300      1956             0    98188  47.4531   \n",
       "\n",
       "          long  sqft_living15  sqft_lot15  \n",
       "16792 -122.173           2450       10220  \n",
       "7648  -122.394           1730        7350  \n",
       "9653  -122.137           2160        7733  \n",
       "21563 -122.041           2240        5123  \n",
       "19437 -122.273           1780        9913  \n",
       "\n",
       "[5 rows x 21 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "house = pd.read_csv('kc_house_data.csv')\n",
    "house.sample(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "      <th>floors</th>\n",
       "      <th>waterfront</th>\n",
       "      <th>view</th>\n",
       "      <th>condition</th>\n",
       "      <th>grade</th>\n",
       "      <th>sqft_above</th>\n",
       "      <th>sqft_basement</th>\n",
       "      <th>sqft_living15</th>\n",
       "      <th>sqft_lot15</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>221900.0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1180</td>\n",
       "      <td>5650</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1180</td>\n",
       "      <td>0</td>\n",
       "      <td>1340</td>\n",
       "      <td>5650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>538000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.25</td>\n",
       "      <td>2570</td>\n",
       "      <td>7242</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>2170</td>\n",
       "      <td>400</td>\n",
       "      <td>1690</td>\n",
       "      <td>7639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>180000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>1.00</td>\n",
       "      <td>770</td>\n",
       "      <td>10000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>770</td>\n",
       "      <td>0</td>\n",
       "      <td>2720</td>\n",
       "      <td>8062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>604000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3.00</td>\n",
       "      <td>1960</td>\n",
       "      <td>5000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>7</td>\n",
       "      <td>1050</td>\n",
       "      <td>910</td>\n",
       "      <td>1360</td>\n",
       "      <td>5000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>510000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.00</td>\n",
       "      <td>1680</td>\n",
       "      <td>8080</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1680</td>\n",
       "      <td>0</td>\n",
       "      <td>1800</td>\n",
       "      <td>7503</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21608</th>\n",
       "      <td>360000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1530</td>\n",
       "      <td>1131</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1530</td>\n",
       "      <td>0</td>\n",
       "      <td>1530</td>\n",
       "      <td>1509</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21609</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>4</td>\n",
       "      <td>2.50</td>\n",
       "      <td>2310</td>\n",
       "      <td>5813</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>2310</td>\n",
       "      <td>0</td>\n",
       "      <td>1830</td>\n",
       "      <td>7200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21610</th>\n",
       "      <td>402101.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1350</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>1020</td>\n",
       "      <td>2007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21611</th>\n",
       "      <td>400000.0</td>\n",
       "      <td>3</td>\n",
       "      <td>2.50</td>\n",
       "      <td>1600</td>\n",
       "      <td>2388</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>8</td>\n",
       "      <td>1600</td>\n",
       "      <td>0</td>\n",
       "      <td>1410</td>\n",
       "      <td>1287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21612</th>\n",
       "      <td>325000.0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.75</td>\n",
       "      <td>1020</td>\n",
       "      <td>1076</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>1020</td>\n",
       "      <td>0</td>\n",
       "      <td>1020</td>\n",
       "      <td>1357</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>21613 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          price  bedrooms  bathrooms  sqft_living  sqft_lot  floors  \\\n",
       "0      221900.0         3       1.00         1180      5650     1.0   \n",
       "1      538000.0         3       2.25         2570      7242     2.0   \n",
       "2      180000.0         2       1.00          770     10000     1.0   \n",
       "3      604000.0         4       3.00         1960      5000     1.0   \n",
       "4      510000.0         3       2.00         1680      8080     1.0   \n",
       "...         ...       ...        ...          ...       ...     ...   \n",
       "21608  360000.0         3       2.50         1530      1131     3.0   \n",
       "21609  400000.0         4       2.50         2310      5813     2.0   \n",
       "21610  402101.0         2       0.75         1020      1350     2.0   \n",
       "21611  400000.0         3       2.50         1600      2388     2.0   \n",
       "21612  325000.0         2       0.75         1020      1076     2.0   \n",
       "\n",
       "       waterfront  view  condition  grade  sqft_above  sqft_basement  \\\n",
       "0               0     0          3      7        1180              0   \n",
       "1               0     0          3      7        2170            400   \n",
       "2               0     0          3      6         770              0   \n",
       "3               0     0          5      7        1050            910   \n",
       "4               0     0          3      8        1680              0   \n",
       "...           ...   ...        ...    ...         ...            ...   \n",
       "21608           0     0          3      8        1530              0   \n",
       "21609           0     0          3      8        2310              0   \n",
       "21610           0     0          3      7        1020              0   \n",
       "21611           0     0          3      8        1600              0   \n",
       "21612           0     0          3      7        1020              0   \n",
       "\n",
       "       sqft_living15  sqft_lot15  \n",
       "0               1340        5650  \n",
       "1               1690        7639  \n",
       "2               2720        8062  \n",
       "3               1360        5000  \n",
       "4               1800        7503  \n",
       "...              ...         ...  \n",
       "21608           1530        1509  \n",
       "21609           1830        7200  \n",
       "21610           1020        2007  \n",
       "21611           1410        1287  \n",
       "21612           1020        1357  \n",
       "\n",
       "[21613 rows x 14 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "house.drop(['id','date','zipcode','lat','long','yr_built','yr_renovated'],axis = 1, inplace = True)\n",
    "house"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = house.iloc[:,1:].to_numpy()\n",
    "y = house['price'].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(21613, 13)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.PathCollection at 0x7fd05deda3d0>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "x = X[:,2]\n",
    "plt.scatter(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on MyLinearRegression1D in module __main__ object:\n",
      "\n",
      "class MyLinearRegression1D(builtins.object)\n",
      " |  The single-variable linear regression estimator -- writing in the style of sklearn package\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  fit(self, x, y)\n",
      " |      Determine the optimal parameters w, b for the input data x and y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array with shape (n_samples,) from training data\n",
      " |         y : 1D numpy array with shape (n_samples,) from training data\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self, with new attributes slope w (float) and intercept b (float)\n",
      " |  \n",
      " |  predict(self, x)\n",
      " |      Predict the output values for the input value x, based on trained parameters\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array from training or test data \n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      returns 1D numpy array of same shape as input, the predicted y value of corresponding x\n",
      " |  \n",
      " |  score(self, x, y)\n",
      " |      Calculate the R-squared on the dataset with input x and y\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |         x : 1D numpy array with shape (n_samples,) from training or test data\n",
      " |         y : 1D numpy array with shape (n_samples,) from training or test data\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      returns float, the R^2 value\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "lreg = MyLinearRegression1D() # initialize the instance of one estimator\n",
    "help(lreg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "lreg.fit(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49286538652201417"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg.score(x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd05e253a90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig = plt.figure(dpi = 100)\n",
    "plt.scatter(x,y)\n",
    "plt.plot(x,lreg.predict(x),'r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model # compare with the scikit learn package\n",
    "lreg_sklearn = linear_model.LinearRegression()\n",
    "lreg_sklearn.fit(x.reshape(-1,1),y) #only accept 2D-array as x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "280.8066899295006 -43867.60153385543\n",
      "[280.80668993] -43867.601533855544\n"
     ]
    }
   ],
   "source": [
    "print(lreg.w,lreg.b)\n",
    "print(lreg_sklearn.coef_, lreg_sklearn.intercept_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49286538652201417"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.score(x.reshape(-1,1),y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on LinearRegression in module sklearn.linear_model._base object:\n",
      "\n",
      "class LinearRegression(sklearn.base.MultiOutputMixin, sklearn.base.RegressorMixin, LinearModel)\n",
      " |  LinearRegression(*, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      " |  \n",
      " |  Ordinary least squares Linear Regression.\n",
      " |  \n",
      " |  LinearRegression fits a linear model with coefficients w = (w1, ..., wp)\n",
      " |  to minimize the residual sum of squares between the observed targets in\n",
      " |  the dataset, and the targets predicted by the linear approximation.\n",
      " |  \n",
      " |  Parameters\n",
      " |  ----------\n",
      " |  fit_intercept : bool, default=True\n",
      " |      Whether to calculate the intercept for this model. If set\n",
      " |      to False, no intercept will be used in calculations\n",
      " |      (i.e. data is expected to be centered).\n",
      " |  \n",
      " |  normalize : bool, default=False\n",
      " |      This parameter is ignored when ``fit_intercept`` is set to False.\n",
      " |      If True, the regressors X will be normalized before regression by\n",
      " |      subtracting the mean and dividing by the l2-norm.\n",
      " |      If you wish to standardize, please use\n",
      " |      :class:`sklearn.preprocessing.StandardScaler` before calling ``fit`` on\n",
      " |      an estimator with ``normalize=False``.\n",
      " |  \n",
      " |  copy_X : bool, default=True\n",
      " |      If True, X will be copied; else, it may be overwritten.\n",
      " |  \n",
      " |  n_jobs : int, default=None\n",
      " |      The number of jobs to use for the computation. This will only provide\n",
      " |      speedup for n_targets > 1 and sufficient large problems.\n",
      " |      ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      " |      ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      " |      for more details.\n",
      " |  \n",
      " |  Attributes\n",
      " |  ----------\n",
      " |  coef_ : array of shape (n_features, ) or (n_targets, n_features)\n",
      " |      Estimated coefficients for the linear regression problem.\n",
      " |      If multiple targets are passed during the fit (y 2D), this\n",
      " |      is a 2D array of shape (n_targets, n_features), while if only\n",
      " |      one target is passed, this is a 1D array of length n_features.\n",
      " |  \n",
      " |  rank_ : int\n",
      " |      Rank of matrix `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  singular_ : array of shape (min(X, y),)\n",
      " |      Singular values of `X`. Only available when `X` is dense.\n",
      " |  \n",
      " |  intercept_ : float or array of shape (n_targets,)\n",
      " |      Independent term in the linear model. Set to 0.0 if\n",
      " |      `fit_intercept = False`.\n",
      " |  \n",
      " |  See Also\n",
      " |  --------\n",
      " |  sklearn.linear_model.Ridge : Ridge regression addresses some of the\n",
      " |      problems of Ordinary Least Squares by imposing a penalty on the\n",
      " |      size of the coefficients with l2 regularization.\n",
      " |  sklearn.linear_model.Lasso : The Lasso is a linear model that estimates\n",
      " |      sparse coefficients with l1 regularization.\n",
      " |  sklearn.linear_model.ElasticNet : Elastic-Net is a linear regression\n",
      " |      model trained with both l1 and l2 -norm regularization of the\n",
      " |      coefficients.\n",
      " |  \n",
      " |  Notes\n",
      " |  -----\n",
      " |  From the implementation point of view, this is just plain Ordinary\n",
      " |  Least Squares (scipy.linalg.lstsq) wrapped as a predictor object.\n",
      " |  \n",
      " |  Examples\n",
      " |  --------\n",
      " |  >>> import numpy as np\n",
      " |  >>> from sklearn.linear_model import LinearRegression\n",
      " |  >>> X = np.array([[1, 1], [1, 2], [2, 2], [2, 3]])\n",
      " |  >>> # y = 1 * x_0 + 2 * x_1 + 3\n",
      " |  >>> y = np.dot(X, np.array([1, 2])) + 3\n",
      " |  >>> reg = LinearRegression().fit(X, y)\n",
      " |  >>> reg.score(X, y)\n",
      " |  1.0\n",
      " |  >>> reg.coef_\n",
      " |  array([1., 2.])\n",
      " |  >>> reg.intercept_\n",
      " |  3.0000...\n",
      " |  >>> reg.predict(np.array([[3, 5]]))\n",
      " |  array([16.])\n",
      " |  \n",
      " |  Method resolution order:\n",
      " |      LinearRegression\n",
      " |      sklearn.base.MultiOutputMixin\n",
      " |      sklearn.base.RegressorMixin\n",
      " |      LinearModel\n",
      " |      sklearn.base.BaseEstimator\n",
      " |      builtins.object\n",
      " |  \n",
      " |  Methods defined here:\n",
      " |  \n",
      " |  __init__(self, *, fit_intercept=True, normalize=False, copy_X=True, n_jobs=None)\n",
      " |      Initialize self.  See help(type(self)) for accurate signature.\n",
      " |  \n",
      " |  fit(self, X, y, sample_weight=None)\n",
      " |      Fit linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
      " |          Training data\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_targets)\n",
      " |          Target values. Will be cast to X's dtype if necessary\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Individual weights for each sample\n",
      " |      \n",
      " |          .. versionadded:: 0.17\n",
      " |             parameter *sample_weight* support to LinearRegression.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : returns an instance of self.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data and other attributes defined here:\n",
      " |  \n",
      " |  __abstractmethods__ = frozenset()\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors inherited from sklearn.base.MultiOutputMixin:\n",
      " |  \n",
      " |  __dict__\n",
      " |      dictionary for instance variables (if defined)\n",
      " |  \n",
      " |  __weakref__\n",
      " |      list of weak references to the object (if defined)\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.RegressorMixin:\n",
      " |  \n",
      " |  score(self, X, y, sample_weight=None)\n",
      " |      Return the coefficient of determination R^2 of the prediction.\n",
      " |      \n",
      " |      The coefficient R^2 is defined as (1 - u/v), where u is the residual\n",
      " |      sum of squares ((y_true - y_pred) ** 2).sum() and v is the total\n",
      " |      sum of squares ((y_true - y_true.mean()) ** 2).sum().\n",
      " |      The best possible score is 1.0 and it can be negative (because the\n",
      " |      model can be arbitrarily worse). A constant model that always\n",
      " |      predicts the expected value of y, disregarding the input features,\n",
      " |      would get a R^2 score of 0.0.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array-like of shape (n_samples, n_features)\n",
      " |          Test samples. For some estimators this may be a\n",
      " |          precomputed kernel matrix or a list of generic objects instead,\n",
      " |          shape = (n_samples, n_samples_fitted),\n",
      " |          where n_samples_fitted is the number of\n",
      " |          samples used in the fitting for the estimator.\n",
      " |      \n",
      " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
      " |          True values for X.\n",
      " |      \n",
      " |      sample_weight : array-like of shape (n_samples,), default=None\n",
      " |          Sample weights.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      score : float\n",
      " |          R^2 of self.predict(X) wrt. y.\n",
      " |      \n",
      " |      Notes\n",
      " |      -----\n",
      " |      The R2 score used when calling ``score`` on a regressor uses\n",
      " |      ``multioutput='uniform_average'`` from version 0.23 to keep consistent\n",
      " |      with default value of :func:`~sklearn.metrics.r2_score`.\n",
      " |      This influences the ``score`` method of all the multioutput\n",
      " |      regressors (except for\n",
      " |      :class:`~sklearn.multioutput.MultiOutputRegressor`).\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from LinearModel:\n",
      " |  \n",
      " |  predict(self, X)\n",
      " |      Predict using the linear model.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      X : array_like or sparse matrix, shape (n_samples, n_features)\n",
      " |          Samples.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      C : array, shape (n_samples,)\n",
      " |          Returns predicted values.\n",
      " |  \n",
      " |  ----------------------------------------------------------------------\n",
      " |  Methods inherited from sklearn.base.BaseEstimator:\n",
      " |  \n",
      " |  __getstate__(self)\n",
      " |  \n",
      " |  __repr__(self, N_CHAR_MAX=700)\n",
      " |      Return repr(self).\n",
      " |  \n",
      " |  __setstate__(self, state)\n",
      " |  \n",
      " |  get_params(self, deep=True)\n",
      " |      Get parameters for this estimator.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      deep : bool, default=True\n",
      " |          If True, will return the parameters for this estimator and\n",
      " |          contained subobjects that are estimators.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      params : mapping of string to any\n",
      " |          Parameter names mapped to their values.\n",
      " |  \n",
      " |  set_params(self, **params)\n",
      " |      Set the parameters of this estimator.\n",
      " |      \n",
      " |      The method works on simple estimators as well as on nested objects\n",
      " |      (such as pipelines). The latter have parameters of the form\n",
      " |      ``<component>__<parameter>`` so that it's possible to update each\n",
      " |      component of a nested object.\n",
      " |      \n",
      " |      Parameters\n",
      " |      ----------\n",
      " |      **params : dict\n",
      " |          Estimator parameters.\n",
      " |      \n",
      " |      Returns\n",
      " |      -------\n",
      " |      self : object\n",
      " |          Estimator instance.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(lreg_sklearn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Motivating Example II: Multi-variable Linear Regression (OLS -- Ordinary Least Square)\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $(x^{(i)},y^{(i)}), i= 1,2,..., N$, this time with $y^{(i)}\\in \\mathbb{R}$ and $x^{(i)}\\in\\mathbb{R}^{p}$, we fit the multi-variable linear function\n",
    "\n",
    "$$y\\approx\\mathbf{f}(x)=\\beta_{0}+\\beta_{1}x_{1}+..+\\beta_{p}x_{p} = \\tilde{x}\\beta,$$  \n",
    "    $$\\tilde{x}=(1,x_{1},..,x_{p})\\in\\mathbb{R}^{1\\times (p+1)},\\beta = (\\beta_{0},\\beta_{1},..,\\beta_{p})^{T}\\in\\mathbb{R}^{(p+1)\\times 1}.$$\n",
    "\n",
    "\n",
    "Here $\\beta$ is called regression coefficients, and $\\beta_{0}$ specially referred to intercept. \n",
    "\n",
    "Using the whole training dataset, we can write as \n",
    "\n",
    "$$Y=\\left(\n",
    " \\begin{matrix}\n",
    "   y^{(1)}\\\\\n",
    "   y^{(2)} \\\\\n",
    "   \\cdots \\\\\n",
    "   y^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\approx\\left(\n",
    "  \\begin{matrix}\n",
    "   \\mathbf{f}(x^{(1)})\\\\\n",
    "   \\mathbf{f}(x^{(2)})\\\\\n",
    "   \\cdots \\\\\n",
    "   \\mathbf{f}(x^{(N)})\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\beta\\\\\n",
    "   \\tilde{x}^{(2)}\\beta\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\\beta\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\\\\n",
    "   \\tilde{x}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\beta = \\tilde{X}\\beta,\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\tilde{X}=\\left(\n",
    "  \\begin{matrix}\n",
    "   1& x_{1}^{(1)} & \\cdots & x_{p}^{(1)}\\\\\n",
    "   1& x_{1}^{(2)} & \\cdots & x_{p}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   1& x_{1}^{(N)} & \\cdots & x_{p}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\n",
    "$$\n",
    "is also called the augmented data matrix.\n",
    "\n",
    "- **Question**: To get unknown $\\beta$, can we directly solve the linear equation $\\tilde{X}\\beta = Y$?\n",
    "\n",
    "- **Answer**: Most time no, because 1) typically there are more equations than variables ($N>>(p+1)$) 2) the linear model is merely the approximation to the real mapping 3) there are noises in the data points -- it's highly possible that there is NO solution at all!\n",
    "\n",
    "- **Strategy**: Instead of solving $\\tilde{X}\\beta = Y$ exactly, we want find $\\beta$ such that $\\tilde{X}\\beta$ is as close as $Y$. \n",
    "\n",
    "### Training the model\n",
    "\n",
    "- With the training dataset, define the loss function $L(\\beta)$ of parameters $\\beta$, which is also called **mean squared error** (MSE) $$L(\\beta)=\\frac{1}{N}\\sum_{i=1}^N\\big(\\hat{y}^{(i)}-y^{(i)}\\big)^2 = \\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\tilde{x}^{(i)}\\beta)^{2},$$\n",
    "where $\\hat{y}^{(i)}$ denotes the predicted value of y at $x^{(i)}$, i.e. $$\\hat{y}^{(i)} = \\beta_{0}+\\beta_{1}x^{(i)}_{1}+..+\\beta_{p}x^{(i)}_{p} = \\tilde{x}^{(i)}\\beta.$$ \n",
    "    \n",
    "    Now the problem becomes $$\\min_{\\beta}L(\\beta),$$ i.e. find the minimizer of a multi-variable (p+1 dimensions) function.\n",
    "\n",
    "\n",
    "- Then find the minimum of loss function -- There are two ways, either by numerical optimization (will be introduced in discussion) or by solving linear systems (introduced below), which is also called the **normal equation** approach.\n",
    "\n",
    "\n",
    "To solve the critical points, we have $\\nabla L(\\beta)=0$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\beta_{0}}&=2\\sum_{i=1}^{N}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\\\\n",
    "\\frac{\\partial L}{\\partial \\beta_{k}}&=2\\sum_{i=1}^{N} x_{k}^{(i)}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\quad k=1,2,..,p.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In Matrix form, it can be expressed as (left as exercise) $$\\tilde{X}^{T}\\tilde{X}\\beta=\\tilde{X}^{T}Y,$$\n",
    "\n",
    "also called the **normal equation** of linear regression. \n",
    "The optimal parameter \n",
    "$\\hat{\\beta}=\\text{argmin} L(\\beta)$\n",
    "is also called the ordinary least square (**OLS**) estimator in statistics community.\n",
    "\n",
    "Then the OLS estimator can be solved as $$\\hat{\\beta}=(\\tilde{X}^{T}\\tilde{X})^{-1}\\tilde{X}^{T}Y.$$\n",
    "\n",
    "**[Geometrical Interpretation](https://en.wikipedia.org/wiki/Ordinary_least_squares)**\n",
    "\n",
    "Denote $\\tilde{X}=(\\tilde{X}_{0},\\tilde{X}_{1},..,\\tilde{X}_{p})$, then $\\tilde{X}\\beta=\\sum_{k=0}^{p}\\beta_{k}\\tilde{X}_{k}$. We require that the residual $Y-\\tilde{X}\\beta$ is vertical to the plane spanned by $\\tilde{X}_{k}$, which yields $$\\tilde{X}_{k}^{T}(Y-\\tilde{X}\\beta)=0,\\quad k = 0,1,...,p$$\n",
    "\n",
    "**Exercise**: Check that when $p=1$, the solution is equivalent to the single-variable regression. \n",
    "\n",
    "### Prediction in Test Data\n",
    "\n",
    "Given the new observation called $x^{(test)}$, we have the prediction as $$\\hat{y}^{(test)}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x^{(test)}_{1}+..+\\hat{\\beta}_{p}x^{(test)}_{p} = \\tilde{x}^{(test)}\\hat{\\beta}.$$\n",
    "\n",
    "### Evaluating the model\n",
    "\n",
    "- MSE: The smaller MSE indicates better performance\n",
    "- R-Squared: The larger $R^{2}$ (closer to 1) indicates better performance. Compared with MSE, R-squared is **dimensionless**, not dependent on the units of variable. \n",
    "\n",
    "$$R^{2} = 1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}} = 1 - \\frac{\\text{MSE}}{\\text{Var}(Y)}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The coding of multi-variable linear regression left as the homework this week. Below we will call the function in sklearn directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6070919341230852"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model # compare with the scikit learn package\n",
    "lreg_sklearn = linear_model.LinearRegression()\n",
    "lreg_sklearn.fit(X,y) \n",
    "lreg_sklearn.score(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-3.46669591e+04, -1.48698203e+04,  1.34078969e+02,  2.67695148e-02,\n",
       "       -3.00709892e+03,  5.85782331e+05,  5.98910372e+04,  5.38601479e+04,\n",
       "        1.00892579e+05,  5.22508348e+01,  8.18281340e+01,  1.10527941e+01,\n",
       "       -7.49796930e-01])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-690582.9505203383"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lreg_sklearn.intercept_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Motivating Example III: Single-variable Polynomial Regression (Special Case of Multivariable Linear Regression)\n",
    "\n",
    "### **Problem**\n",
    "Given the *training dataset* $(x^{(i)},y^{(i)}), i= 1,2,..., N$, this time with $y^{(i)}\\in \\mathbb{R}$ and $x^{(i)}\\in\\mathbb{R}$, we fit the single-variable polynomial function of $p$-th order\n",
    "\n",
    "$$y\\approx f(x)=w_{0}+w_{1}x+w_{2}x^{2}+...+w_{p}x^{p}$$  \n",
    "\n",
    "**Remark:** A basic conclusion in numerical analysis is that with N points, we can have a polynomial of order (N-1) that fits every point perfectly.\n",
    " \n",
    "### **Strategy**\n",
    "Single-variable **polynomial regression** is a special case of multi-variable **linear** regression, because we can construct a dataset of $p$ variables by defining each row as $(x,x^{2}, ..., x^{p})$ for each observation at $x$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "___\n",
    "##  Machine Learning: Overview of the whole picture\n",
    "Possible hierarchies of machine learning concepts:\n",
    "\n",
    "- **Problems**: Supervised Learning(Regression,Classification), Unsupervised Learning (Dimension Reduction, Clustering), Reinforcement Learning (Not covered in this course)\n",
    "\n",
    "\n",
    "- **Models**: \n",
    "    - (Supervised) Linear Regression, Logistic Regression, K-Nearest Neighbor (kNN) Classification/Regression, Decision Tree, Random Forest, Support Vector Machine, Ensemble Method, Neural Network...\n",
    "    - (Unsupervised) K-means,Hierachical Clustering, Principle Component Analysis, Manifold Learning (MDS, IsoMap, Diffusion Map, tSNE), Auto Encoder...\n",
    "    \n",
    "\n",
    "- **Algorithms**: Gradient Descent, Stochastic Gradient Descent (SGD), Back Propagation (BP),Expectation–Maximization (EM)...\n",
    "    \n",
    "    \n",
    "For the same **problem**, there may exist multiple **models** to discribe it. Given the specific **model**, there might be many different **algorithms** to solve it.\n",
    "\n",
    "Why there is so much diversity? The following two fundamental principles of machine learning may provide theoretical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[Bias-Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)**: Simple models -- large bias, low variance. Complex models -- low bias, large variance\n",
    "\n",
    "**[No Free Lunch Theorem](https://analyticsindiamag.com/what-are-the-no-free-lunch-theorems-in-data-science/#:~:text=Once%20Upon%20A%20Time,that%20they%20brought%20a%20drink)**: (in plain language) There is no one model that works best for every problem. (more quantitatively) Any two models are equivalent when their performance averaged across all possible problems. --Even true for [optimization algorithms](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Extensions of OLS:  MLE, Regularization, Ridge Regression and LASSO\n",
    "\n",
    "*Note: The detailed mathematical derivations below are optional material. You only need to know (for quiz/exam):*\n",
    "\n",
    "1) what is the relation between MLE (most likelihood estimation) and the loss function in OLS regression (ordinary least-square)\n",
    "\n",
    "2) the basic concepts of Ridge regression and LASSO ;\n",
    "\n",
    "3) where does the additional regularization terms in the loss function of Ridge and LASSO come from ;\n",
    "\n",
    "4) which model has the best performance on training/test dataset? (or is there any theoretical guarantee?)\n",
    "\n",
    "\n",
    "### Most Likelihood Estimation (MLE) and loss function in OLS\n",
    "We already known what the loss function looks like in OLS. Here we first provide a mathematical explanation of this loss function from the perspective of \n",
    "Most Likelihood Estimation (MLE).\n",
    "\n",
    "Recall that in linear regression, our **model assumption** is \n",
    "\n",
    "$$y^{(i)}=\\tilde{x}^{(i)}\\beta+\\epsilon^{(i)}, i = 1,2,.., N$$ \n",
    "\n",
    "Now we further **assume** that residuals or errors $\\epsilon^{(i)}$ are as independent Gaussian random variables with identical distribution $\\mathcal{N}(0,\\sigma^{2})$ which has mean 0 and standard deviation $\\sigma$.\n",
    "\n",
    "From the density function of Gaussian distribution, the prabability to observe $\\epsilon^{(i)}$ within the small interval $[z,z+\\Delta z]$ is roughly $$\\mathbb{P}(z<\\epsilon^{(i)}<z+\\Delta z) = \\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{z^2}{2\\sigma^2}})\\Delta z.$$\n",
    "\n",
    "From the data, we know indeed $z=y^{(i)}-\\tilde{x}^{(i)}\\beta$. Therefore, given $x^{(i)}$ as fixed, the probability density (likelihood) to observe $y^{(i)}$ is roughly $$l(y^{(i)}|x^{(i)},\\beta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{(y^{(i)}-\\tilde{x}^{(i)}\\beta)^2}{2\\sigma^2}}).$$\n",
    "\n",
    "Using the *independence* assumption, the overall likelihood to observe the response data $y^{i}(i=1,2,...,N)$ is \n",
    "\n",
    "$$\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)})=\\prod_{i=1}^{N}l(y^{(i)}|x^{(i)},\\beta)$$\n",
    "\n",
    "The famous **Maximum Likelihood Estimation (MLE)** theory in statistics **assumes** that we aim to find the unknown parameter $\\beta$ that maximizes the $\\mathcal{P}(\\beta;x^{(i)},y^{(i)},1\\leq i\\leq N)$ by treating $x^{(i)}$ and $y^{(i)}$ as fixed numbers. \n",
    "\n",
    "Equivalently, as the function of $\\beta$, we can maximize $$\\ln \\mathcal{P}= \\sum_{i=1}^{N}\\ln l(y^{(i)}|\\beta,x^{(i)}).$$ \n",
    "\n",
    "By removing the constants, we finally arrives at the **minimization** problem of $L^{2}$ loss function (whose difference with **MSE -- mean squared error** is only up to the factor 1/N)\n",
    "$$L(\\beta)=\\sum_{i=1}^{N}(y^{(i)}-\\tilde{x}^{(i)}\\beta)^{2}= ||Y-\\tilde{X}\\beta||_{2}^2.$$\n",
    "\n",
    "\n",
    "### MAP (instead of MLE) Estimation in Bayesian Statistics\n",
    "\n",
    "**Recall** the likelihood function -- we interpret it as the probability of observing the response data, given the parameter $\\beta$ as fixed, i.e. conditional probability\n",
    "\n",
    "$$\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)})=\\prod_{i=1}^{N}l(y^{(i)}|x^{(i)},\\beta)$$\n",
    "\n",
    "Now we take a bayesian approach -- assume $\\beta$ is the random variable with **prior distirbution** $\\mathcal{P}(\\beta)$. Then the **posterior distribution** of $\\beta$ given the data is  \n",
    "\n",
    "\n",
    "$$\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)\\propto \\mathcal{P}(\\beta)\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)}).$$\n",
    "\n",
    "\n",
    "The **Bayesian** estimation aims to maximaze the posterior distribution. It is formally termed as **Maximum A-Posteriori Estimation (MAP)**. Note that \n",
    "\n",
    "\n",
    "$$\\text{argmax}_{\\beta}\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)=\\text{argmax}_{\\beta}\\ln\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)$$\n",
    "\n",
    "\n",
    "- Case 1: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-x^{2}), i\\geq 1$ is Gaussian-like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda||\\beta||_{2}^{2}.$$\n",
    "\n",
    "    here $||\\beta||_{2}^{2}=\\sum_{i=1}^{p}\\beta_{i}^{2}.$\n",
    "    This is called **Ridge Regression**. \n",
    "    \n",
    "    \n",
    "- Case 2: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-|x|), i\\geq 1$ is double-exponential like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda\\sum_{i=1}^{p}|\\beta_{i}|$$\n",
    "    \n",
    "    This is called [**LASSO Regression**](https://en.wikipedia.org/wiki/Lasso_(statistics)).\n",
    "    \n",
    "    \n",
    "    \n",
    "In general, these additional terms are called the **regularization terms**. In statistics, regularization is equivalent to Bayesian prior. Here $\\lambda$ is the adjustable parameter in algorithm --  its choice is empirical while sometimes very important for model performance (where the word \"alchemy\" arises in machine learning) Roughly it controls the **complexity** of the model:\n",
    "\n",
    "- If $\\lambda\\to\\infty$, we have $\\beta_{i}\\to 0 (i\\geq 1)$ and $\\beta_{0} = \\bar{y}$.\n",
    "- If $\\lambda\\to 0 $, it will yield the same results with OLS.\n",
    "\n",
    "Why control the complexity? Recall the bias-variance tradeoff -- sometimes reduce the complexity of model **might** help to improve performace in test dataset.\n",
    "\n",
    "### Algorithm consideration## \n",
    "\n",
    "The optimization for ridge regression is similar to OLS -- try to derive the analytical solution your self. The optimization for LASSO is [non-trival](https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf) and is the important topic in convex optimization. \n",
    "\n",
    "### Prediction in Test Data\n",
    "\n",
    "Now from the same training dataset, we have three $\\beta$ estimated from three different models, namely $\\hat{\\beta}^{OLS},\\hat{\\beta}^{Ridge},\\hat{\\beta}^{Lasso}$ because they are the minimizers of three different loss functions.\n",
    "\n",
    "Given the new observation called $x^{(test)}$, the formal expression of predictions from different methods are the same $$\\hat{y}^{(test)}=\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x^{(test)}_{1}+..+\\hat{\\beta}_{p}x^{(test)}_{p} = \\tilde{x}^{(test)}\\hat{\\beta}.$$\n",
    "\n",
    "The **only** difference is what $\\hat{\\beta}$ we use. Of course, the corresponded prediction values are also different.\n",
    "\n",
    "### Model Performance Evaluation\n",
    "\n",
    "   - Mean Square Error (MSE) -- the lower, the better (in test data):  $\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}$\n",
    "   - R-squared (coefficient of determination, $R^{2}$) -- the larger, the better (in test data): $1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}}$\n",
    "   \n",
    "\n",
    "Question: What about on the training dataset?\n",
    "\n",
    "Conclusion: **By definition**, compared with Ridge or LASSO regression, OLS **will be sure** to have the smallest MSE (hence largest $R^{2}$) on **training dataset**. Think why!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Diabetes Dataset\n",
    "\n",
    "We use the [scikit-learn package](https://scikit-learn.org/stable/index.html) to load the data and run regression. More tutorials about linear models can be [found here](https://scikit-learn.org/stable/modules/linear_model.html).\n",
    "\n",
    "Data from [this paper](https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf) by Professor [Robert Tibshirani et al](https://statweb.stanford.edu/~tibs/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y= datasets.load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function load_diabetes in module sklearn.datasets._base:\n",
      "\n",
      "load_diabetes(*, return_X_y=False, as_frame=False)\n",
      "    Load and return the diabetes dataset (regression).\n",
      "    \n",
      "    ==============   ==================\n",
      "    Samples total    442\n",
      "    Dimensionality   10\n",
      "    Features         real, -.2 < x < .2\n",
      "    Targets          integer 25 - 346\n",
      "    ==============   ==================\n",
      "    \n",
      "    Read more in the :ref:`User Guide <diabetes_dataset>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    return_X_y : bool, default=False.\n",
      "        If True, returns ``(data, target)`` instead of a Bunch object.\n",
      "        See below for more information about the `data` and `target` object.\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "    \n",
      "    as_frame : bool, default=False\n",
      "        If True, the data is a pandas DataFrame including columns with\n",
      "        appropriate dtypes (numeric). The target is\n",
      "        a pandas DataFrame or Series depending on the number of target columns.\n",
      "        If `return_X_y` is True, then (`data`, `target`) will be pandas\n",
      "        DataFrames or Series as described below.\n",
      "    \n",
      "        .. versionadded:: 0.23\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    data : :class:`~sklearn.utils.Bunch`\n",
      "        Dictionary-like object, with the following attributes.\n",
      "    \n",
      "        data : {ndarray, dataframe} of shape (442, 10)\n",
      "            The data matrix. If `as_frame=True`, `data` will be a pandas\n",
      "            DataFrame.\n",
      "        target: {ndarray, Series} of shape (442,)\n",
      "            The regression target. If `as_frame=True`, `target` will be\n",
      "            a pandas Series.\n",
      "        feature_names: list\n",
      "            The names of the dataset columns.\n",
      "        frame: DataFrame of shape (442, 11)\n",
      "            Only present when `as_frame=True`. DataFrame with `data` and\n",
      "            `target`.\n",
      "    \n",
      "            .. versionadded:: 0.23\n",
      "        DESCR: str\n",
      "            The full description of the dataset.\n",
      "        data_filename: str\n",
      "            The path to the location of the data.\n",
      "        target_filename: str\n",
      "            The path to the location of the target.\n",
      "    \n",
      "    (data, target) : tuple if ``return_X_y`` is True\n",
      "    \n",
      "        .. versionadded:: 0.18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(datasets.load_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the training and test dataset by random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(397, 10)\n",
      "(45,)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.model_selection._split:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(ShuffleSplit().split(X, y))`` and application to input data\n",
      "    into a single call for splitting (and optionally subsampling) data in a\n",
      "    oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "        Allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "    test_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the proportion\n",
      "        of the dataset to include in the test split. If int, represents the\n",
      "        absolute number of test samples. If None, the value is set to the\n",
      "        complement of the train size. If ``train_size`` is also None, it will\n",
      "        be set to 0.25.\n",
      "    \n",
      "    train_size : float or int, default=None\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState instance, default=None\n",
      "        Controls the shuffling applied to the data before applying the split.\n",
      "        Pass an int for reproducible output across multiple function calls.\n",
      "        See :term:`Glossary <random_state>`.\n",
      "    \n",
      "    \n",
      "    shuffle : bool, default=True\n",
      "        Whether or not to shuffle the data before splitting. If shuffle=False\n",
      "        then stratify must be None.\n",
      "    \n",
      "    stratify : array-like, default=None\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the class labels.\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length=2 * len(arrays)\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            If the input is sparse, the output will be a\n",
      "            ``scipy.sparse.csr_matrix``. Else, output type is the same as the\n",
      "            input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.model_selection import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "    \n",
      "    >>> train_test_split(y, shuffle=False)\n",
      "    [[0, 1, 2], [3, 4]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Square (OLS) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression()"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "reg_ols = linear_model.LinearRegression()\n",
    "reg_ols.fit(X_train,y_train) # train the parameters in training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['__abstractmethods__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getstate__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__setstate__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_abc_impl',\n",
       " '_check_n_features',\n",
       " '_decision_function',\n",
       " '_estimator_type',\n",
       " '_get_param_names',\n",
       " '_get_tags',\n",
       " '_more_tags',\n",
       " '_preprocess_data',\n",
       " '_repr_html_',\n",
       " '_repr_html_inner',\n",
       " '_repr_mimebundle_',\n",
       " '_residues',\n",
       " '_set_intercept',\n",
       " '_validate_data',\n",
       " 'coef_',\n",
       " 'copy_X',\n",
       " 'fit',\n",
       " 'fit_intercept',\n",
       " 'get_params',\n",
       " 'intercept_',\n",
       " 'n_features_in_',\n",
       " 'n_jobs',\n",
       " 'normalize',\n",
       " 'predict',\n",
       " 'rank_',\n",
       " 'score',\n",
       " 'set_params',\n",
       " 'singular_']"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(reg_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  19.92576904, -262.55453086,  509.19112446,  336.09693678,\n",
       "       -849.29530342,  480.22076125,  120.68418641,  236.71853501,\n",
       "        716.61035542,   70.41045019])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([143.06621271, 177.70923973, 134.80159283, 288.66523611,\n",
       "       123.58429291,  96.64399491, 252.70865552, 183.51563317,\n",
       "        93.96508916, 109.83316004,  98.04648824, 168.61502622,\n",
       "        58.09759262, 206.5896178 , 102.4078438 , 130.25693511,\n",
       "       218.0570909 , 245.9207401 , 193.24351477, 214.36945188,\n",
       "       208.82778064,  90.55665059,  74.15304744, 187.1216387 ,\n",
       "       156.36442036, 157.46376883, 184.17736744, 177.18027887,\n",
       "        52.24263585, 110.66673778, 174.05918425,  90.89850309,\n",
       "       133.07968763, 183.22988596, 173.93725211, 189.85248233,\n",
       "       125.86458581, 121.53390004, 148.94895292,  60.82842472,\n",
       "        76.36312191, 106.40220555, 162.20473499, 153.15077269,\n",
       "       174.23003255])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred_ols = reg_ols.predict(X_test) # generate predictions in test dataset\n",
    "y_pred_ols"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2743.8800467688443 0.5514251914993505\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "R2_ols =  reg_ols.score(X_test,y_test) # the R-squared value -- how good is the fitting in test dataset?\n",
    "print(mse_ols,R2_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  21.70557246 -252.8105591   507.97196544  328.21420703 -280.47609687\n",
      "   37.89517179 -127.46013757  163.28415598  497.87046059   77.00701528]\n",
      "2735.677504142067 0.5527661590071533\n"
     ]
    }
   ],
   "source": [
    "reg_ridge = linear_model.Ridge(alpha=.02) # alpha is proportional to the lambda above -- only up to the constant\n",
    "reg_ridge.fit(X_train,y_train)\n",
    "print(reg_ridge.coef_)\n",
    "\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "R2_ridge =  reg_ridge.score(X_test,y_test)\n",
    "print(mse_ridge,R2_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0.         -173.66792464  510.09537263  286.77901824  -64.43166585\n",
      "   -0.         -226.79324775    0.          453.3557073    44.84749075]\n",
      "2636.3323830017634 0.569007291247414\n"
     ]
    }
   ],
   "source": [
    "reg_lasso = linear_model.Lasso(alpha=0.1) # alpha is proportional to the lambda above -- only up to the constant\n",
    "reg_lasso.fit(X_train,y_train)\n",
    "print(reg_lasso.coef_)\n",
    "\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "R2_lasso =  reg_lasso.score(X_test,y_test)\n",
    "print(mse_lasso,R2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5125152248773208\n",
      "0.5102072320833588\n",
      "0.5024076500883519\n"
     ]
    }
   ],
   "source": [
    "print(reg_ols.score(X_train,y_train)) # note that we calculate score on TRAINING dataset\n",
    "print(reg_ridge.score(X_train,y_train))\n",
    "print(reg_lasso.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, OLS has the smallest MSE (largest R-squared) on **training dataset**. What about on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "alphas = np.logspace(-5, -1, 20)\n",
    "for alpha in alphas:\n",
    "    reg_lasso.set_params(alpha=alpha) # change the parameter of reg_lasso\n",
    "    reg_lasso.fit(X_train, y_train)\n",
    "    train_errors.append(reg_lasso.score(X_train, y_train))\n",
    "    test_errors.append(reg_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x7fd05fd07810>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(dpi=100)\n",
    "plt.semilogx(alphas,train_errors,label = 'train R2')\n",
    "plt.semilogx(alphas,test_errors,label = 'test R2')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Therefore, a good model in training dataset does not mean it's a good model in the final test dataset. Then how can we use the best model (i.e. model with best regularization parameters)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
    "\n",
    "What if we don't know the true labels in test, but the performance in test is so important to us so that we really want to select a model with greater confidence with traning dataset?\n",
    "\n",
    "As discussed previously, we can use traning dataset to make 10 \"quizzes\" (each \"quiz\" is called a validation dataset), and let the three models to compete based on the 10 \"competitions\". This is called 10-fold cross-validation.\n",
    "\n",
    "For the more detailed discussion and distinguishment between training, validation and test datasets, you can refer to this [wikipedia link](https://en.wikipedia.org/wiki/Training,_validation,_and_test_sets#Test_dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_lasso = cross_val_score(reg_lasso, X_train, y_train, cv=10) # cross-validation function in sklearn\n",
    "scores_ridge = cross_val_score(reg_ridge, X_train, y_train, cv=10)\n",
    "scores_ols = cross_val_score(reg_ols, X_train, y_train, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.24777555 0.59326777 0.47897959 0.5352791  0.32317178 0.47569164\n",
      " 0.6518041  0.56942576 0.25184587 0.36446431]\n",
      "[0.24342237 0.57522902 0.52325584 0.53031117 0.34021405 0.48194162\n",
      " 0.6585968  0.57423334 0.24263773 0.33362724]\n",
      "[0.23604669 0.57037558 0.53700808 0.52611281 0.34264557 0.49282279\n",
      " 0.66256801 0.57878559 0.19975324 0.34375095]\n"
     ]
    }
   ],
   "source": [
    "print(scores_lasso)\n",
    "print(scores_ridge)\n",
    "print(scores_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function cross_val_score in module sklearn.model_selection._validation:\n",
      "\n",
      "cross_val_score(estimator, X, y=None, *, groups=None, scoring=None, cv=None, n_jobs=None, verbose=0, fit_params=None, pre_dispatch='2*n_jobs', error_score=nan)\n",
      "    Evaluate a score by cross-validation\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    estimator : estimator object implementing 'fit'\n",
      "        The object to use to fit the data.\n",
      "    \n",
      "    X : array-like of shape (n_samples, n_features)\n",
      "        The data to fit. Can be for example a list, or an array.\n",
      "    \n",
      "    y : array-like of shape (n_samples,) or (n_samples, n_outputs),             default=None\n",
      "        The target variable to try to predict in the case of\n",
      "        supervised learning.\n",
      "    \n",
      "    groups : array-like of shape (n_samples,), default=None\n",
      "        Group labels for the samples used while splitting the dataset into\n",
      "        train/test set. Only used in conjunction with a \"Group\" :term:`cv`\n",
      "        instance (e.g., :class:`GroupKFold`).\n",
      "    \n",
      "    scoring : str or callable, default=None\n",
      "        A str (see model evaluation documentation) or\n",
      "        a scorer callable object / function with signature\n",
      "        ``scorer(estimator, X, y)`` which should return only\n",
      "        a single value.\n",
      "    \n",
      "        Similar to :func:`cross_validate`\n",
      "        but only a single metric is permitted.\n",
      "    \n",
      "        If None, the estimator's default scorer (if available) is used.\n",
      "    \n",
      "    cv : int, cross-validation generator or an iterable, default=None\n",
      "        Determines the cross-validation splitting strategy.\n",
      "        Possible inputs for cv are:\n",
      "    \n",
      "        - None, to use the default 5-fold cross validation,\n",
      "        - int, to specify the number of folds in a `(Stratified)KFold`,\n",
      "        - :term:`CV splitter`,\n",
      "        - An iterable yielding (train, test) splits as arrays of indices.\n",
      "    \n",
      "        For int/None inputs, if the estimator is a classifier and ``y`` is\n",
      "        either binary or multiclass, :class:`StratifiedKFold` is used. In all\n",
      "        other cases, :class:`KFold` is used.\n",
      "    \n",
      "        Refer :ref:`User Guide <cross_validation>` for the various\n",
      "        cross-validation strategies that can be used here.\n",
      "    \n",
      "        .. versionchanged:: 0.22\n",
      "            ``cv`` default value if None changed from 3-fold to 5-fold.\n",
      "    \n",
      "    n_jobs : int, default=None\n",
      "        The number of CPUs to use to do the computation.\n",
      "        ``None`` means 1 unless in a :obj:`joblib.parallel_backend` context.\n",
      "        ``-1`` means using all processors. See :term:`Glossary <n_jobs>`\n",
      "        for more details.\n",
      "    \n",
      "    verbose : int, default=0\n",
      "        The verbosity level.\n",
      "    \n",
      "    fit_params : dict, default=None\n",
      "        Parameters to pass to the fit method of the estimator.\n",
      "    \n",
      "    pre_dispatch : int or str, default='2*n_jobs'\n",
      "        Controls the number of jobs that get dispatched during parallel\n",
      "        execution. Reducing this number can be useful to avoid an\n",
      "        explosion of memory consumption when more jobs get dispatched\n",
      "        than CPUs can process. This parameter can be:\n",
      "    \n",
      "            - None, in which case all the jobs are immediately\n",
      "              created and spawned. Use this for lightweight and\n",
      "              fast-running jobs, to avoid delays due to on-demand\n",
      "              spawning of the jobs\n",
      "    \n",
      "            - An int, giving the exact number of total jobs that are\n",
      "              spawned\n",
      "    \n",
      "            - A str, giving an expression as a function of n_jobs,\n",
      "              as in '2*n_jobs'\n",
      "    \n",
      "    error_score : 'raise' or numeric, default=np.nan\n",
      "        Value to assign to the score if an error occurs in estimator fitting.\n",
      "        If set to 'raise', the error is raised.\n",
      "        If a numeric value is given, FitFailedWarning is raised. This parameter\n",
      "        does not affect the refit step, which will always raise the error.\n",
      "    \n",
      "        .. versionadded:: 0.20\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    scores : array of float, shape=(len(list(cv)),)\n",
      "        Array of scores of the estimator for each run of the cross validation.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> from sklearn import datasets, linear_model\n",
      "    >>> from sklearn.model_selection import cross_val_score\n",
      "    >>> diabetes = datasets.load_diabetes()\n",
      "    >>> X = diabetes.data[:150]\n",
      "    >>> y = diabetes.target[:150]\n",
      "    >>> lasso = linear_model.Lasso()\n",
      "    >>> print(cross_val_score(lasso, X, y, cv=3))\n",
      "    [0.33150734 0.08022311 0.03531764]\n",
      "    \n",
      "    See Also\n",
      "    ---------\n",
      "    :func:`sklearn.model_selection.cross_validate`:\n",
      "        To run cross-validation on multiple metrics and also to return\n",
      "        train scores, fit times and score times.\n",
      "    \n",
      "    :func:`sklearn.model_selection.cross_val_predict`:\n",
      "        Get predictions from each split of cross-validation for diagnostic\n",
      "        purposes.\n",
      "    \n",
      "    :func:`sklearn.metrics.make_scorer`:\n",
      "        Make a scorer from a performance metric or loss function.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso</th>\n",
       "      <th>ols</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.236047</td>\n",
       "      <td>0.243422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.593268</td>\n",
       "      <td>0.570376</td>\n",
       "      <td>0.575229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.478980</td>\n",
       "      <td>0.537008</td>\n",
       "      <td>0.523256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.535279</td>\n",
       "      <td>0.526113</td>\n",
       "      <td>0.530311</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.323172</td>\n",
       "      <td>0.342646</td>\n",
       "      <td>0.340214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.475692</td>\n",
       "      <td>0.492823</td>\n",
       "      <td>0.481942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.651804</td>\n",
       "      <td>0.662568</td>\n",
       "      <td>0.658597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.569426</td>\n",
       "      <td>0.578786</td>\n",
       "      <td>0.574233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.251846</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.242638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.364464</td>\n",
       "      <td>0.343751</td>\n",
       "      <td>0.333627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      lasso       ols     ridge\n",
       "0  0.247776  0.236047  0.243422\n",
       "1  0.593268  0.570376  0.575229\n",
       "2  0.478980  0.537008  0.523256\n",
       "3  0.535279  0.526113  0.530311\n",
       "4  0.323172  0.342646  0.340214\n",
       "5  0.475692  0.492823  0.481942\n",
       "6  0.651804  0.662568  0.658597\n",
       "7  0.569426  0.578786  0.574233\n",
       "8  0.251846  0.199753  0.242638\n",
       "9  0.364464  0.343751  0.333627"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "scores_all = pd.DataFrame({\"lasso\": scores_lasso,\"ols\": scores_ols, \"ridge\":scores_ridge})\n",
    "scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides mean and standard deviation, we can also use the [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 600x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "sns.set_theme()\n",
    "fig, ax = plt.subplots(dpi=100)\n",
    "sns.boxplot(data = scores_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lasso</th>\n",
       "      <th>ols</th>\n",
       "      <th>ridge</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.449171</td>\n",
       "      <td>0.448987</td>\n",
       "      <td>0.450347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.144468</td>\n",
       "      <td>0.157290</td>\n",
       "      <td>0.148598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.247776</td>\n",
       "      <td>0.199753</td>\n",
       "      <td>0.242638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.333495</td>\n",
       "      <td>0.342922</td>\n",
       "      <td>0.335274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.477336</td>\n",
       "      <td>0.509468</td>\n",
       "      <td>0.502599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.560889</td>\n",
       "      <td>0.562034</td>\n",
       "      <td>0.563253</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>0.651804</td>\n",
       "      <td>0.662568</td>\n",
       "      <td>0.658597</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           lasso        ols      ridge\n",
       "count  10.000000  10.000000  10.000000\n",
       "mean    0.449171   0.448987   0.450347\n",
       "std     0.144468   0.157290   0.148598\n",
       "min     0.247776   0.199753   0.242638\n",
       "25%     0.333495   0.342922   0.335274\n",
       "50%     0.477336   0.509468   0.502599\n",
       "75%     0.560889   0.562034   0.563253\n",
       "max     0.651804   0.662568   0.658597"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores_all.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of course, the final judgement is still in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.569007291247414"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_lasso.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5527661590071533"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ridge.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5514251914993505"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg_ols.score(X_test,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise: Use cross-validation to select the `alpha` parameter in LASSO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reference Reading Suggestions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ISLR: Chapter 2,3,6\n",
    "- ESL: Chapter 1,2,3\n",
    "- PML: Chapter 1,2,3,4,7,11\n",
    "- DL: Chapter 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
