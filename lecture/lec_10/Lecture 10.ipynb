{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 10 Introduction to Machine Learning and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Overview of the whole picture\n",
    "\n",
    "Possible hierarchies of machine learning concepts:\n",
    "\n",
    "- **Problems**: Supervised Learning(Regression,Classification), Unsupervised Learning (Dimension Reduction, Clustering), Reinforcement Learning (Not covered in this course)\n",
    "\n",
    "\n",
    "- **Models**: \n",
    "    - (Supervised) Linear Regression, Logistic Regression, K-Nearest Neighbor (kNN) Classification/Regression, Decision Tree, Random Forest, Support Vector Machine, Ensemble Method, Neural Network...\n",
    "    - (Unsupervised) K-means,Hierachical Clustering, Principle Component Analysis, Manifold Learning (MDS, IsoMap, Diffusion Map, tSNE), Auto Encoder...\n",
    "    \n",
    "\n",
    "- **Algorithms**: Gradient Descent, Stochastic Gradient Descent (SGD), Back Propagation (BP),Expectationâ€“Maximization (EM)...\n",
    "    \n",
    "    \n",
    "For the same **problem**, there may exist multiple **models** to discribe it. Given the specific **model**, there might be many different **algorithms** to solve it.\n",
    "\n",
    "Why there is so much diversity? The following two fundamental principles of machine learning may provide theoretical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**[Bias-Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)**: Simple models -- large bias, low variance. Complex models -- low bias, large variance\n",
    "\n",
    "**[No Free Lunch Theorem](https://analyticsindiamag.com/what-are-the-no-free-lunch-theorems-in-data-science/#:~:text=Once%20Upon%20A%20Time,that%20they%20brought%20a%20drink)**: (in plain language) There is no one model that works best for every problem. (more quantitatively) Any two models are equivalent when their performance averaged across all possible problems. --Even true for [optimization algorithms](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression\n",
    "\n",
    "Recall the basic task of **supervised learning**: given the *training dataset* $(x^{(i)},y^{(i)}), i= 1,2,..., N$ with $y^{(i)}\\in \\mathbb{R}^{q}$ (for simplicity, assume $q=1$) denotes the *labels*, the supervised learning aims to find a mapping $y\\approx\\mathbf{f}(x):\\mathbb{R}^{p}\\to\\mathbb{R}$ that we can use it to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Setup\n",
    "\n",
    "#### Model assumption 1: Linear Mapping Assumption.\n",
    "\n",
    "$$y\\approx\\mathbf{f}(x)=\\beta_{0}+\\beta_{1}x_{1}+..+\\beta_{p}x_{p} = \\tilde{x}\\beta,$$  \n",
    "    $$\\tilde{x}=(1,x_{1},..,x_{p})\\in\\mathbb{R}^{1\\times (p+1)},\\beta = (\\beta_{0},\\beta_{1},..,\\beta_{p})^{T}\\in\\mathbb{R}^{(p+1)\\times 1}.$$\n",
    "\n",
    "\n",
    "Here $\\beta$ is called regression coefficients, and $\\beta_{0}$ specially referred to intercept. \n",
    "\n",
    "Using the whole training dataset, we can write as \n",
    "\n",
    "$$Y=\\left(\n",
    " \\begin{matrix}\n",
    "   y^{(1)}\\\\\n",
    "   y^{(2)} \\\\\n",
    "   \\cdots \\\\\n",
    "   y^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\approx\\left(\n",
    "  \\begin{matrix}\n",
    "   \\mathbf{f}(x^{(1)})\\\\\n",
    "   \\mathbf{f}(x^{(2)})\\\\\n",
    "   \\cdots \\\\\n",
    "   \\mathbf{f}(x^{(N)})\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\beta\\\\\n",
    "   \\tilde{x}^{(2)}\\beta\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\\beta\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\\\\n",
    "   \\tilde{x}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\beta = \\tilde{X}\\beta,\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\tilde{X}=\\left(\n",
    "  \\begin{matrix}\n",
    "   1& \\tilde{x}_{1}^{(1)} & \\cdots & \\tilde{x}_{p}^{(1)}\\\\\n",
    "   1& \\tilde{x}_{1}^{(2)} & \\cdots & \\tilde{x}_{p}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   1& \\tilde{x}_{1}^{(N)} & \\cdots & \\tilde{x}_{p}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\n",
    "$$\n",
    "is also called the augmented data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model assumption 2: Gaussian Residual Assumption ($L^{2}$ loss assumption)\n",
    "$$y^{(i)}=\\tilde{x}^{(i)}\\beta+\\epsilon^{(i)}, i = 1,2,.., N$$\n",
    "The residuals or errors $\\epsilon^{(i)}$ are **assumed** as independent Gaussian random variables with identical distribution $\\mathcal{N}(0,\\sigma^{2})$ which has mean 0 and standard deviation $\\sigma$.\n",
    "\n",
    "From the density function of Gaussian distribution, the prabability to observe $\\epsilon^{(i)}$ within the small interval $[z,z+\\Delta z]$ is roughly $$\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{z^2}{2\\sigma^2}})\\Delta z.$$\n",
    "\n",
    "From the data, we know indeed $z=y^{(i)}-\\tilde{x}^{(i)}\\beta$. Therefore, the probability density (likelihood) to observe $(x^{(i)},y^{(i)})$ is roughly $$l(x^{(i)},y^{(i)},\\beta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{(y^{(i)}-\\tilde{x}^{(i)}\\beta)^2}{2\\sigma^2}}).$$\n",
    "\n",
    "Using the *independence* assumption, the overall likelihood to observe the data is \n",
    "\n",
    "$$\\mathcal{L}(\\beta;x^{(i)},y^{(i)},1\\leq i\\leq N)=\\prod_{i=1}^{N}l(x^{(i)},y^{(i)},\\beta)$$\n",
    "\n",
    "The famous **Maximum Likelihood Estimation** theory in statistics **assumes** that we aim to find the unknown parameter $\\beta$ that maximizes the $\\mathcal{L}(\\beta;x^{(i)},y^{(i)},1\\leq i\\leq N)$ by treating $x^{(i)}$ and $y^{(i)}$ as fixed numbers. \n",
    "\n",
    "Equivalently, as the function of $\\beta$, we can maximize $\\ln \\mathcal{L}(\\beta;x^{(i)},y^{(i)},1\\leq i\\leq N)= \\sum_{i=1}^{N}\\ln l(x^{(i)},y^{(i)},\\beta)$. \n",
    "\n",
    "By removing the constants, we finally arrives at the **minimization** problem of $L^{2}$ loss function \n",
    "$$L(\\beta)=\\sum_{i=1}^{N}(y^{(i)}-\\tilde{x}^{(i)}\\beta)^{2}= ||Y-\\tilde{X}\\beta||_{2}^2.$$\n",
    "\n",
    "The optimal parameter \n",
    "$$\\hat{\\beta}=\\text{argmin} L(\\beta)$$\n",
    "is also called the ordinary least square (OLS) estimator in statistics community."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
