{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Lecture 10 Introduction to Machine Learning and Linear Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "##  Overview of the whole picture\n",
    "\n",
    "Possible hierarchies of machine learning concepts:\n",
    "\n",
    "- **Problems**: Supervised Learning(Regression,Classification), Unsupervised Learning (Dimension Reduction, Clustering), Reinforcement Learning (Not covered in this course)\n",
    "\n",
    "\n",
    "- **Models**: \n",
    "    - (Supervised) Linear Regression, Logistic Regression, K-Nearest Neighbor (kNN) Classification/Regression, Decision Tree, Random Forest, Support Vector Machine, Ensemble Method, Neural Network...\n",
    "    - (Unsupervised) K-means,Hierachical Clustering, Principle Component Analysis, Manifold Learning (MDS, IsoMap, Diffusion Map, tSNE), Auto Encoder...\n",
    "    \n",
    "\n",
    "- **Algorithms**: Gradient Descent, Stochastic Gradient Descent (SGD), Back Propagation (BP),Expectationâ€“Maximization (EM)...\n",
    "    \n",
    "    \n",
    "For the same **problem**, there may exist multiple **models** to discribe it. Given the specific **model**, there might be many different **algorithms** to solve it.\n",
    "\n",
    "Why there is so much diversity? The following two fundamental principles of machine learning may provide theoretical insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**[Bias-Variance Trade-off](https://towardsdatascience.com/understanding-the-bias-variance-tradeoff-165e6942b229)**: Simple models -- large bias, low variance. Complex models -- low bias, large variance\n",
    "\n",
    "**[No Free Lunch Theorem](https://analyticsindiamag.com/what-are-the-no-free-lunch-theorems-in-data-science/#:~:text=Once%20Upon%20A%20Time,that%20they%20brought%20a%20drink)**: (in plain language) There is no one model that works best for every problem. (more quantitatively) Any two models are equivalent when their performance averaged across all possible problems. --Even true for [optimization algorithms](https://en.wikipedia.org/wiki/No_free_lunch_in_search_and_optimization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Linear Regression\n",
    "\n",
    "Recall the basic task of **supervised learning**: given the *training dataset* $(x^{(i)},y^{(i)}), i= 1,2,..., N$ with $y^{(i)}\\in \\mathbb{R}^{q}$ (for simplicity, assume $q=1$) denotes the *labels*, the supervised learning aims to find a mapping $y\\approx\\mathbf{f}(x):\\mathbb{R}^{p}\\to\\mathbb{R}$ that we can use it to make predictions on the test dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Model Setup\n",
    "\n",
    "#### Model assumption 1: Linear Mapping Assumption.\n",
    "\n",
    "$$y\\approx\\mathbf{f}(x)=\\beta_{0}+\\beta_{1}x_{1}+..+\\beta_{p}x_{p} = \\tilde{x}\\beta,$$  \n",
    "    $$\\tilde{x}=(1,x_{1},..,x_{p})\\in\\mathbb{R}^{1\\times (p+1)},\\beta = (\\beta_{0},\\beta_{1},..,\\beta_{p})^{T}\\in\\mathbb{R}^{(p+1)\\times 1}.$$\n",
    "\n",
    "\n",
    "Here $\\beta$ is called regression coefficients, and $\\beta_{0}$ specially referred to intercept. \n",
    "\n",
    "Using the whole training dataset, we can write as \n",
    "\n",
    "$$Y=\\left(\n",
    " \\begin{matrix}\n",
    "   y^{(1)}\\\\\n",
    "   y^{(2)} \\\\\n",
    "   \\cdots \\\\\n",
    "   y^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\approx\\left(\n",
    "  \\begin{matrix}\n",
    "   \\mathbf{f}(x^{(1)})\\\\\n",
    "   \\mathbf{f}(x^{(2)})\\\\\n",
    "   \\cdots \\\\\n",
    "   \\mathbf{f}(x^{(N)})\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\beta\\\\\n",
    "   \\tilde{x}^{(2)}\\beta\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\\beta\n",
    "  \\end{matrix} \n",
    "\\right)=\\left(\n",
    "  \\begin{matrix}\n",
    "   \\tilde{x}^{(1)}\\\\\n",
    "   \\tilde{x}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   \\tilde{x}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\\beta = \\tilde{X}\\beta,\n",
    "$$\n",
    "\n",
    "where \n",
    "$$\n",
    "\\tilde{X}=\\left(\n",
    "  \\begin{matrix}\n",
    "   1& x_{1}^{(1)} & \\cdots & x_{p}^{(1)}\\\\\n",
    "   1& x_{1}^{(2)} & \\cdots & x_{p}^{(2)}\\\\\n",
    "   \\cdots \\\\\n",
    "   1& x_{1}^{(N)} & \\cdots & x_{p}^{(N)}\n",
    "  \\end{matrix} \n",
    "\\right)\n",
    "$$\n",
    "is also called the augmented data matrix."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "#### Model assumption 2: Gaussian Residual Assumption ($L^{2}$ loss assumption)\n",
    "$$y^{(i)}=\\tilde{x}^{(i)}\\beta+\\epsilon^{(i)}, i = 1,2,.., N$$\n",
    "The residuals or errors $\\epsilon^{(i)}$ are **assumed** as independent Gaussian random variables with identical distribution $\\mathcal{N}(0,\\sigma^{2})$ which has mean 0 and standard deviation $\\sigma$.\n",
    "\n",
    "From the density function of Gaussian distribution, the prabability to observe $\\epsilon^{(i)}$ within the small interval $[z,z+\\Delta z]$ is roughly $$\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{z^2}{2\\sigma^2}})\\Delta z.$$\n",
    "\n",
    "From the data, we know indeed $z=y^{(i)}-\\tilde{x}^{(i)}\\beta$. Therefore, given $x^{(i)}$ as fixed, the probability density (likelihood) to observe $y^{(i)}$ is roughly $$l(y^{(i)}|x^{(i)},\\beta)=\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp({-\\frac{(y^{(i)}-\\tilde{x}^{(i)}\\beta)^2}{2\\sigma^2}}).$$\n",
    "\n",
    "Using the *independence* assumption, the overall likelihood to observe the response data $y^{i}(i=1,2,...,N)$ is \n",
    "\n",
    "$$\\mathcal{L}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)})=\\prod_{i=1}^{N}l(y^{(i)}|x^{(i)},\\beta)$$\n",
    "\n",
    "The famous **Maximum Likelihood Estimation** theory in statistics **assumes** that we aim to find the unknown parameter $\\beta$ that maximizes the $\\mathcal{L}(\\beta;x^{(i)},y^{(i)},1\\leq i\\leq N)$ by treating $x^{(i)}$ and $y^{(i)}$ as fixed numbers. \n",
    "\n",
    "Equivalently, as the function of $\\beta$, we can maximize $$\\ln \\mathcal{L}= \\sum_{i=1}^{N}\\ln l(y^{(i)}|\\beta,x^{(i)}).$$ \n",
    "\n",
    "By removing the constants, we finally arrives at the **minimization** problem of $L^{2}$ loss function \n",
    "$$L(\\beta)=\\sum_{i=1}^{N}(y^{(i)}-\\tilde{x}^{(i)}\\beta)^{2}= ||Y-\\tilde{X}\\beta||_{2}^2.$$\n",
    "\n",
    "The optimal parameter \n",
    "$$\\hat{\\beta}=\\text{argmin} L(\\beta)$$\n",
    "is also called the ordinary least square (OLS) estimator in statistics community.\n",
    "\n",
    "We also have the prediction $$\\hat{y}^{(i)}=\\tilde{x}^{(i)}\\hat{\\beta}.$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithm: Normal Equation\n",
    "\n",
    "To solve the critical points, we have $\\nabla L(\\beta)=0$.\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\frac{\\partial L}{\\partial \\beta_{0}}&=2\\sum_{i=1}^{N}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\\\\n",
    "\\frac{\\partial L}{\\partial \\beta_{k}}&=2\\sum_{i=1}^{N} x_{k}^{(i)}(\\tilde{x}^{(i)}\\beta-y^{(i)})=0,\\quad k=1,2,..,p.\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "In Matrix form, it can be expressed as (left as exercise) $$\\tilde{X}^{T}\\tilde{X}\\beta=\\tilde{X}^{T}Y,$$\n",
    "\n",
    "also called the **normal equation** of linear regression. Then the OLS estimator can be solved as $$\\hat{\\beta}=(\\tilde{X}^{T}\\tilde{X})^{-1}\\tilde{X}^{T}Y.$$\n",
    "\n",
    "**[Geometrical Interpretation](https://en.wikipedia.org/wiki/Ordinary_least_squares)**\n",
    "\n",
    "Denote $\\tilde{X}=(\\tilde{X}_{0},\\tilde{X}_{1},..,\\tilde{X}_{p})$, then $\\tilde{X}\\beta=\\sum_{k=0}^{p}\\beta_{k}\\tilde{X}_{k}$. We require that the residual $Y-\\tilde{X}\\beta$ is vertical to the plane spanned by $\\tilde{X}_{k}$, which yields $$\\tilde{X}_{k}^{T}(Y-\\tilde{X}\\beta)=0,\\quad k = 0,1,...,p$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extensions: Regularization, Ridge Regression and LASSO\n",
    "\n",
    "Recall the likelihood function -- we interpret it as the probability of observing the response data, given the parameter $\\beta$ as fixed, i.e. conditional probability\n",
    "$$\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)})=\\prod_{i=1}^{N}l(y^{(i)}|x^{(i)},\\beta)$$\n",
    "\n",
    "Now we take a bayesian approach -- assume $\\beta$ is the random variable with **prior distirbution** $\\mathcal{P}(\\beta)$. Then the **posterior distribution** of $\\beta$ given the data is  $$\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)\\propto \\mathcal{P}(\\beta)\\mathcal{P}(y^{(i)},1\\leq i\\leq N|\\beta,x^{(i)}).$$\n",
    "\n",
    "The **Bayesian** estimation aims to maximaze the posterior distribution. Note that \n",
    "\n",
    "$$\\text{argmax}_{\\beta}\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)=\\text{argmax}_{\\beta}\\ln\\mathcal{P}(\\beta|x^{(i)},y^{(i)},1\\leq i\\leq N)$$\n",
    "\n",
    "- Case 1: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-x^{2})$ is Gaussian-like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda||\\beta||_{2}^{2}.$$\n",
    "\n",
    "here $||\\beta||_{2}^{2}=\\sum_{i=0}^{p}\\beta_{i}^{2}.$\n",
    "    This is called **Ridge Regression**. \n",
    "    \n",
    "Here $\\lambda$ is the adjustable parameter in algorithm --  its choice is empirical while sometimes very important for model performance (where the word \"alchemy\" arises in machine learning!!!)\n",
    "\n",
    "\n",
    "- Case 2: The prior distribution $\\mathcal{P}(\\beta_{i}=x)\\propto \\exp(-|x|)$ is double-exponential like, and different $\\beta_{i}$ are independent. Now the minimization problem becomes \n",
    "\n",
    "    $$\\min_{\\beta} ||Y-\\tilde{X}\\beta||_{2}^2+\\lambda\\sum_{i=0}^{p}|\\beta_{i}|$$\n",
    "    \n",
    "    This is called [**LASSO Regression**](https://en.wikipedia.org/wiki/Lasso_(statistics)).\n",
    "    \n",
    "In general, these additional terms are called the **regularization terms**. In statistics, regularization is equivalent to Bayesian prior.\n",
    "\n",
    "Algorithm consideration: The optimization for ridge regression is similar to OLS -- try to derive the analytical solution your self. The optimization for LASSO is [non-trival](https://www.cs.ubc.ca/~schmidtm/Documents/2005_Notes_Lasso.pdf) and is the important topic in convex optimization. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Diabetes Dataset\n",
    "\n",
    "We use the [scikit-learn package](https://scikit-learn.org/stable/index.html) to load the data and run regression. More tutorials about linear models can be [found here](https://scikit-learn.org/stable/modules/linear_model.html).\n",
    "\n",
    "Data from [this paper](https://web.stanford.edu/~hastie/Papers/LARS/LeastAngle_2002.pdf) by Professor [Robert Tibshirani et al](https://statweb.stanford.edu/~tibs/index.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "X,y= datasets.load_diabetes(return_X_y = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(datasets.load_diabetes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the training and test dataset by random splitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(train_test_split)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ordinary Least Square (OLS) Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "reg_ols = linear_model.LinearRegression()\n",
    "reg_ols.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(reg_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ols.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_ols = reg_ols.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluation:\n",
    "   - Mean Square Error (MSE) -- the lower, the better (in test data):  $\\frac{1}{N}\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}$\n",
    "   - R-squared (coefficient of determination, $R^{2}$) -- the larger, the better (in test data): $1 - \\frac{\\sum_{i=1}^{N}(y^{(i)}-\\hat{y}^{(i)})^{2}}{\\sum_{i=1}^{N}(y^{(i)}-\\bar{y})^{2}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mse_ols = mean_squared_error(y_test, y_pred_ols)\n",
    "R2_ols =  reg_ols.score(X_test,y_test)\n",
    "print(mse_ols,R2_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_ridge = linear_model.Ridge(alpha=.02)\n",
    "reg_ridge.fit(X_train,y_train)\n",
    "print(reg_ridge.coef_)\n",
    "y_pred_ridge = reg_ridge.predict(X_test)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "R2_ridge =  reg_ridge.score(X_test,y_test)\n",
    "print(mse_ridge,R2_ridge)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_lasso = linear_model.Lasso(alpha=.05)\n",
    "reg_lasso.fit(X_train,y_train)\n",
    "print(reg_lasso.coef_)\n",
    "\n",
    "y_pred_lasso = reg_lasso.predict(X_test)\n",
    "mse_lasso = mean_squared_error(y_test, y_pred_lasso)\n",
    "R2_lasso =  reg_lasso.score(X_test,y_test)\n",
    "print(mse_lasso,R2_lasso)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reg_ols.score(X_train,y_train))\n",
    "print(reg_ridge.score(X_train,y_train))\n",
    "print(reg_lasso.score(X_train,y_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By definition, OLS has the smallest MSE (largest R-squared) on **training dataset**. What about on the test dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "train_errors = list()\n",
    "test_errors = list()\n",
    "alphas = np.logspace(-5, -1, 20)\n",
    "for alpha in alphas:\n",
    "    reg_lasso.set_params(alpha=alpha) # change the parameter of reg_lasso\n",
    "    reg_lasso.fit(X_train, y_train)\n",
    "    train_errors.append(reg_lasso.score(X_train, y_train))\n",
    "    test_errors.append(reg_lasso.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(dpi=100)\n",
    "plt.semilogx(alphas,train_errors,label = 'train R2')\n",
    "plt.semilogx(alphas,test_errors,label = 'test R2')\n",
    "plt.xlabel('alpha')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### [Cross Validation](https://scikit-learn.org/stable/modules/cross_validation.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores_lasso = cross_val_score(reg_lasso, X, y, cv=10)\n",
    "scores_ridge = cross_val_score(reg_ridge, X, y, cv=10)\n",
    "scores_ols = cross_val_score(reg_ols, X, y, cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(scores_lasso)\n",
    "print(scores_ridge)\n",
    "print(scores_ols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(cross_val_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "scores_all = pd.DataFrame({\"lasso\": scores_lasso,\"ols\": scores_ols, \"ridge\":scores_ridge})\n",
    "scores_all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides mean and standard deviation, we can also use the [boxplot](https://towardsdatascience.com/understanding-boxplots-5e2df7bcbd51) to visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(dpi=100)\n",
    "sns.boxplot(data = scores_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
